{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gzip as gz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic TensorFlow usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2])\n",
    "b = tf.constant([3, 4])\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function to read EMNIST data into numpy array\n",
    "def read_mnist(images_path, labels_path):\n",
    "    with gz.open(labels_path, 'rb') as labelsFile:\n",
    "        labels = np.frombuffer(labelsFile.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gz.open(images_path,'rb') as imagesFile:\n",
    "        length = len(labels)\n",
    "        # Load flat 28x28 px images (784 px), and convert them to 28x28 px\n",
    "        features = np.frombuffer(imagesFile.read(), dtype=np.uint8, offset=16) \\\n",
    "                        .reshape(length, 784) \\\n",
    "                        .reshape(length, 28, 28)\n",
    "        features = features.astype(float)\n",
    "        flip = features[:,:, ::-1,...]       # note that images are flipped\n",
    "        features = np.rot90(flip, 1, (1,2))  # and rotated 90deg\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Test and training data and labels need to be read from the binary files and converted into a form for easy use. Luckily, this data is small enough to fit in memory.\n",
    "This data set has some pecularities, like images are flipped and rotated 90%. This needs to be accounts for as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "train['features'], train['labels'] = read_mnist('data/emnist-bymerge-train-images-idx3-ubyte.gz', 'data/emnist-bymerge-train-labels-idx1-ubyte.gz')\n",
    "test['features'], test['labels'] = read_mnist('data/emnist-bymerge-test-images-idx3-ubyte.gz', 'data/emnist-bymerge-test-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read the labels so that directories can be named appropriately\n",
    "def map_labels(label_mappings=\"data/emnist-bymerge-mapping.txt\"):\n",
    "    labels_dict = {}\n",
    "    with open(label_mappings, 'rb') as f:\n",
    "        # each row of the file has the label first and ascii code next\n",
    "        for line in f:\n",
    "            items = line.split()\n",
    "            # note that data is in bytes, so need to convert\n",
    "            labels_dict.update({int(items[0]): chr(int(items[1]))})\n",
    "    return labels_dict\n",
    "\n",
    "# convenience function to display a grid of random images\n",
    "def display_images(features, labels, mapping):\n",
    "    # Now, lets try and generate some sample images to see what the data looks like\n",
    "    fig=plt.figure(figsize=(9, 9))  # show 8in X 8in image\n",
    "    columns = 4  # 4 images per row\n",
    "    rows = 5  # lay out images on 5 rows\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img_id = np.random.randint(0, features.shape[0])  # max number of images, from prev section\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        img_data = features[img_id].squeeze()  #,:,:]\n",
    "        plt.title('Label: %d Char: %s' % ( labels[img_id], mapping[labels[img_id]]))\n",
    "        plt.imshow(img_data, cmap='gray')\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "It is always useful to plot a few samples of the data, especially for visual applications, to build intuition and ensure things are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = map_labels()\n",
    "display_images(train['features'], train['labels'], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Model\n",
    "Keras makes it really easy to specify model architectures at a high level. In TensorFlow 2.0, further simplfications have been made, so that the code is really clean and simple to follow. In the chapter, the model architecture was specified to have 256 units in first hidden layer, 128 in the second one. The hidden layers had ReLU as the activation function and the output layer had softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(name='First FC DL Model')\n",
    "# flatten from a 28px X 28px into 784 inputs.\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "# Add another:\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "# Add a softmax layer with 47 output units:\n",
    "model.add(layers.Dense(47, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to check, especially for complex models\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SideBar: Example of losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the values for binary classification. Each element is a prediction\n",
    "actual = [0., 0., 1., 1.]\n",
    "predicted = [0.95, 0.88, 1., 0.]\n",
    "\n",
    "# different loss definitions\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae_loss = mae(actual, predicted)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse_loss = mse(actual, predicted)\n",
    "\n",
    "print('MAE Loss: ', mae_loss.numpy())  # Loss: 0.7075\n",
    "print('MSE Loss: ', mse_loss.numpy())  # Loss: 0.6692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical cross entropy loss\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss = cce(\n",
    "  [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\n",
    "  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n",
    "print('Categorical Cross-entropy Loss: ', loss.numpy())  # Loss: 0.3239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple straightforward training\n",
    "one_hot_labels = tf.one_hot(train['labels'], 47)\n",
    "history = model.fit(train['features'], one_hot_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['accuracy', 'loss'], loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and evaluation against the test set\n",
    "one_hot_test = tf.one_hot(test['labels'], 47)\n",
    "model.evaluate(test['features'], one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  # sklearn has pre-built modules\n",
    "\n",
    "predictions = model.predict_on_batch(test['features'])  # generates a probability for each label\n",
    "y_pred = np.argmax(predictions, axis=1)  # choose the one with the highest value as the output label\n",
    "\n",
    "print('Confusion Matrix')\n",
    "confusion = confusion_matrix(test['labels'], y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty plotting the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convenience function to enable reuse\n",
    "def plot_confusion_matrix(confusion):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    im = ax.imshow(keras.utils.normalize(confusion), cmap=\"YlGn\")\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, shrink=0.75)\n",
    "    cbar.ax.set_ylabel(\"Percentage of Samples\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(47))\n",
    "    ax.set_yticks(np.arange(47))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(list(labels.values()))  # NOTE: The axis labels are hard coded here\n",
    "    ax.set_yticklabels(list(labels.values()))\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "    ax.set_xticks(np.arange(47+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(47+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Lets plot the confusion matrix now\n",
    "plot_confusion_matrix(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_features = keras.utils.normalize(train['features'])\n",
    "norm_test_features = keras.utils.normalize(test['features'])\n",
    "one_hot_labels = tf.one_hot(train['labels'], 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with normalization of inputs\n",
    "model.fit(norm_train_features, one_hot_labels, epochs=10, batch_size=256)  # includes a step of data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and evaluation\n",
    "model.evaluate(norm_test_features, tf.one_hot(test['labels'], 47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_on_batch(test['features'])  # generates a probability for each label\n",
    "y_pred = np.argmax(predictions, axis=1)  # choose the one with the highest value as the output label\n",
    "\n",
    "print('Confusion Matrix')\n",
    "confusion = confusion_matrix(test['labels'], y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model was trained with data normalization and test set was not normalized\n",
    "# and evaluation\n",
    "model.evaluate(test['features'], tf.one_hot(test['labels'], 47))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(norm_test_features, batch_size=32, verbose=1)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "report = classification_report(test['labels'], predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the last trained model using data normalization\n",
    "model.save(\"emnist_fc_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(\"emnist_fc_model.h5\")\n",
    "new_model.evaluate(norm_test_features, tf.one_hot(test['labels'], 47))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
